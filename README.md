# Fat-Protein-Calculation-AI

### AI-based Analysis of Meat Fat and Protein Content

## Background

- In recent years, the interest in fitness and personal health management has significantly increased, leading to greater attention towards dietary management.
- However, bulk purchases of meat often lack accurate labeling for fat and protein content.
- Estimating fat and protein amounts becomes particularly challenging when meat is portioned and repackaged.

## Architecture

![Image](https://github.com/user-attachments/assets/93ea2e0b-21a7-42fc-b2e6-a2464f490429)

- The first U-Net model segments the meat from the image.
- The second U-Net model separates the fat from the meat.
- Hand length is used as a scale marker to calculate pixels per centimeter.
- Additionally, a depth estimation model generates a depth map from images, which is converted to absolute thickness using the hand length scale.
- Subsequently, these values are input into the equations below to calculate the meat's fat content, and protein content is calculated by subtracting fat and water content.
  ![Image](https://github.com/user-attachments/assets/2b340c78-bab3-48ab-a384-2a59ffa2429d)
  ![Image](https://github.com/user-attachments/assets/98976063-f31f-4dd4-9dee-a07bc599e041)

## Method

We used SegFormer, a Transformer-based semantic segmentation model, after testing indicated superior performance compared to U-Net, DeepLabV3, and HRNet.

The tables below present performance for meat segmentation and fat segmentation models respectively.

| Model            | IoU Score | Dice Coefficient | Pixel Accuracy |
| ---------------- | --------- | ---------------- | -------------- |
| DeepLabV3        | 0.9528    | 0.9757           | 0.9866         |
| HRNet            | 0.9426    | 0.9702           | 0.9856         |
| SegFormer        | 0.9676    | 0.9834           | 0.9904         |
| U-Net (baseline) | 0.9588    | 0.9789           | 0.9882         |

| Model            | IoU Score | Dice Coefficient | Pixel Accuracy |
| ---------------- | --------- | ---------------- | -------------- |
| DeepLabV3        | 0.1722    | 0.2803           | 0.9337         |
| HRNet            | 0.2182    | 0.3361           | 0.9492         |
| SegFormer        | 0.2384    | 0.3641           | 0.9577         |
| U-Net (baseline) | 0.2322    | 0.3571           | 0.9560         |

## Dataset

- We collected 80 images directly from the internet for keywords like "raw beef" and "raw pork".
- Labeled meat areas using Labelme.
- For fat labeling, images were converted to grayscale, and threshold values were applied (utilizing the whiteness of fat).
- Using data augmentation techniques (flip, rotation) on the 80 beef and 79 pork images collected, we created 560 beef images and 553 pork images.
- Dataset was divided into train, validation, and test sets with a 0.7:0.15:0.15 ratio for model training and validation.

## Result

### 1. Separate Training for Pork and Beef

The upper table represents meat segmentation, and the lower table represents fat segmentation.

| Meat | IoU Score | Dice coefficient | Pixel Accuracy |
| ---- | --------- | ---------------- | -------------- |
| Beef | 0.9669    | 0.9830           | 0.9902         |
| Pork | 0.9640    | 0.9816           | 0.9901         |

| Meat | IoU Score | Dice coefficient | Pixel Accuracy |
| ---- | --------- | ---------------- | -------------- |
| Beef | 0.2233    | 0.3453           | 0.9522         |
| Pork | 0.3160    | 0.4491           | 0.9598         |

### 2. Combined Training for Pork and Beef

| Meat | IoU Score | Dice coefficient | Pixel Accuracy |
| ---- | --------- | ---------------- | -------------- |
| Beef | 0.9669    | 0.9830           | 0.9902         |
| Pork | 0.9640    | 0.9816           | 0.9901         |

| Meat | IoU Score | Dice coefficient | Pixel Accuracy |
| ---- | --------- | ---------------- | -------------- |
| Beef | 0.2233    | 0.3453           | 0.9522         |
| Pork | 0.3160    | 0.4491           | 0.9598         |

The meat segmentation model performed better when trained jointly, but fat segmentation performance was better when trained separately. Joint training may enhance generalization for meat segmentation but not fat segmentation.

## Example

- Meat segmentation model (Beef)
  ![Image](https://github.com/user-attachments/assets/8b448e89-e16c-4729-9c4d-d1fe38c0ce90)
- Meat segmentation model (Pork)
  ![Image](https://github.com/user-attachments/assets/85a833b8-cb3d-4b92-979a-a17211d26c4b)
- Fat segmentation model (Beef)
  ![Image](https://github.com/user-attachments/assets/75bbca74-d843-4ac2-a907-abbf675bee25)
- Fat segmentation model (Pork)
  ![Image](https://github.com/user-attachments/assets/62f333cd-173f-4271-a39a-0b1f618a185b)
- Mediapipe library detecting hand landmarks
  ![Image](https://github.com/user-attachments/assets/e42ebd7e-a5fc-41e4-bf0f-1238ca863544)
  - Distance from fingertip to wrist is measured to calculate pixels per centimeter.
- Depth map generated by MiDaS model
  ![Image](https://github.com/user-attachments/assets/1bd6f73e-74bc-4f5c-95f4-9ae8b61afbae)
  - Brighter regions indicate greater thickness.
  - The relative thickness is converted to absolute thickness using pixels per centimeter, allowing estimation of meat thickness.
  - Using this pipeline, we obtained 26.40g of fat and 16.19g of protein from an image.

## Discussion

- Improved methods for more accurate fat segmentation are needed.
- User-friendly interfaces like mobile applications should be provided.

## Conclusion

This project developed a pipeline leveraging machine learning to estimate meat fat and protein content using images and hand length. Despite dataset limitations affecting generalization, the results demonstrate practicality and effectiveness.

This model has potential as a personal dietary management tool and can be developed into a meat quality control system in the food industry.

## Reference

- https://dl.acm.org/doi/fullHtml/10.1145/3575879.3575975#fig5
- https://arxiv.org/abs/2105.15203
- https://mediapipe.readthedocs.io/en/latest/solutions/hands.html
- https://github.com/isl-org/MiDaS
